{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeType(final):\n",
    "    tps = []\n",
    "    for i in range(len(final)-1):\n",
    "        if final[i+1][1] != '' and final[i][1] == '':\n",
    "            tps.append(i-len(tps))\n",
    "    return tps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergeType([(['the', 'demise', 'of', '<Entity>'], ''), (['What', 'disease', 'led', 'to'], 'Disease')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = [(['the', 'demise', 'of', '<Entity>'], ''), (['What', 'disease', 'led', 'to'], 'Disease')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergeType(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['What', 'disease', 'led', 'to'], 'Disease')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.tree import ParentedTree\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from hierarchy import Hierarchy\n",
    "import ast\n",
    "import string\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost', port=9000)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "specificStopWords = {'Give', 'Show', 'List', 'Name', 'Tell', 'many', 'much', 'all', 'whose', 'often', 'also', 'Count', 'Find', 'amongst', 'Amongst', 'among', 'Among', 'would'}\n",
    "whwords = {'How', 'Which', 'Who', 'Whose', 'Whom', 'When', 'Where', 'What', 'Give', 'Show', 'List', 'Name', 'Tell', 'Count', 'Find', 'what'}\n",
    "newstopwords = [line.strip() for line in open('stopwords').readlines()]\n",
    "yesnowords = {'is', 'are', 'were', 'was', 'do', 'does', 'did', 'has', 'have', 'had'}\n",
    "types = [(t.strip().upper(),t.strip()) for t in open('hierarchy.txt').readlines()]\n",
    "misstypes = [tuple(t.strip().split(',')) for t in open('./misstypes').readlines()]\n",
    "typedict = dict(types+misstypes)\n",
    "misstypedict = dict(misstypes)\n",
    "del typedict['TYPE']\n",
    "table = dict.fromkeys(string.punctuation)\n",
    "del table['<']\n",
    "del table['>']\n",
    "table = str.maketrans(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findQType(line):\n",
    "    line = line[0].upper() + line[1:]\n",
    "    lineSplitOrig = line.split()\n",
    "    if lineSplitOrig[1] == 'which':\n",
    "        lineSplitOrig[1] = 'Which'\n",
    "    elif lineSplitOrig[1] == 'what':\n",
    "        lineSplitOrig[1] = 'What'\n",
    "    line = ' '.join(lineSplitOrig)\n",
    "    ptree = ParentedTree.fromstring(nlp.parse(line))\n",
    "    firstw = lineSplitOrig[0].lower()\n",
    "    if firstw == 'who' or firstw == 'whom' or firstw == 'whose':\n",
    "        qt = 'Person'\n",
    "    elif firstw == 'when':\n",
    "        qt = 'Time'\n",
    "    elif firstw == 'where':\n",
    "        qt = 'Place'\n",
    "    else:\n",
    "        qt = ''\n",
    "    lineSplit = [x for x in lineSplitOrig if x.lower() not in newstopwords]\n",
    "    linetypes = {}\n",
    "    j = 0\n",
    "    while j < len(lineSplit):\n",
    "        for k in range(len(lineSplit), j, -1):\n",
    "            catl = lineSplit[j:k]\n",
    "            cat = ''.join(map(lambda x:lemmatizer.lemmatize(x.lower(), 'n').upper(), catl))\n",
    "            # for l in range(j, k+1):\n",
    "            #     wl = lineSplit[l]\n",
    "            #     cat += lemmatizer.lemmatize(wl.lower(), 'n').upper()\n",
    "            #     catl.append(wl)\n",
    "            if cat in typedict:\n",
    "                word = ' '.join(catl)\n",
    "                currtype = typedict[cat]\n",
    "                if word in linetypes or\\\n",
    "                    len(catl) == 1 and (word == 'place' and lineSplitOrig[lineSplitOrig.index(word)-1] in ['take', 'takes', 'took']\\\n",
    "                        or findPOS(ptree, word) != 'N'\\\n",
    "                        or lineSplit.index(word) != len(lineSplit) - 1 and findPOS(ptree, lineSplitOrig[lineSplitOrig.index(word)+1]) == 'N'):\n",
    "                    continue\n",
    "                if len(catl) > 1:\n",
    "                    linetypes[currtype] = currtype\n",
    "                    line = line.replace(word, currtype)\n",
    "                else:\n",
    "                    linetypes[word] = currtype\n",
    "                j = k\n",
    "                break\n",
    "        j += 1\n",
    "    return line, qt, linetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPOS(ptree, w):\n",
    "    w = w.translate(table)\n",
    "    if w in ptree.leaves():\n",
    "        return ptree[ptree.leaf_treeposition(ptree.leaves().index(w))[:-1]].label()[0]\n",
    "    else:\n",
    "        return 'P'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('From Which party is the politician who was selected in <E>?', '', {'party': 'PoliticalParty'})\n"
     ]
    }
   ],
   "source": [
    "line = 'From which party is the politician who was selected  in <E>?'\n",
    "print(findQType(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'politician'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('politician', 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'POLITICIAN' in typedict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 8, 7, 6, 5, 4, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(10,2,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('~/word2vec/GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem(word):\n",
    "    if word in model.vocab:\n",
    "        return word\n",
    "    else:\n",
    "        vlem = lemmatizer.lemmatize(word, 'v')\n",
    "        if vlem != word and vlem in model.vocab:\n",
    "            return vlem\n",
    "        nlem = lemmatizer.lemmatize(word, 'n')\n",
    "        if nlem != word and nlem in model.vocab:\n",
    "            return nlem\n",
    "        alem = lemmatizer.lemmatize(word, 'a')\n",
    "        if alem != word and alem in model.vocab:\n",
    "            return alem\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineSplit = ['working']\n",
    "lvs = np.transpose(normalize(np.stack([model[lem(ls)] for ls in lineSplit if lem(ls) != ''])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['birth'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predL = [['employer'],['founded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13316257, 0.22012392], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.average(np.max(np.dot(normalize(np.stack([model[lem(ps)] for ps in predSplit if lem(ps) != ''])), lvs), axis=1)) for predSplit in predL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = PyDictionary.PyDictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangyifan/anaconda3/lib/python3.7/site-packages/PyDictionary/utils.py:5: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 5 of the file /Users/yangyifan/anaconda3/lib/python3.7/site-packages/PyDictionary/utils.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  return BeautifulSoup(requests.get(url).text)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Noun': ['a person or firm that employs workers']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.meaning('employer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'own' in model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.tree import ParentedTree\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import numpy as np\n",
    "from SPARQLWrapper import SPARQLWrapper,JSON\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from GstoreConnector import GstoreConnector\n",
    "from hierarchy import Hierarchy\n",
    "import string\n",
    "import ast\n",
    "import json\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost', port=9000)\n",
    "sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
    "# sparql = GstoreConnector(\"dbpedia.gstore-pku.com\", 80, \"endpoint\", \"123\")\n",
    "specificStopWords = {'Give', 'Show', 'List', 'Name', 'Tell', 'many', 'much', 'all', 'whose', 'often', 'also', 'Count', 'Find', 'amongst', 'among', 'would'}\n",
    "whwords = {'How', 'Which', 'Who', 'When', 'Where', 'What'}\n",
    "timetypes = {'Time','Year'}\n",
    "literaltypes = {'Population', 'Name'}\n",
    "timexmltypes = {'duration', 'dateTime', 'time', 'date', 'gYearMonth', 'gYear', 'gMonthDay', 'gDay', 'gMonth'}\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('~/word2vec/GoogleNews-vectors-negative300.bin',binary=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "table = dict.fromkeys(string.punctuation)\n",
    "del table['<']\n",
    "del table['>']\n",
    "table = str.maketrans(table)\n",
    "newstopwords = {line.strip() for line in open('stopwords').readlines()}\n",
    "otherwords = {'<Entity>', '<E1>', '<E2>', 'someone', 'everyone'}\n",
    "newstopwords = newstopwords | otherwords | specificStopWords\n",
    "\n",
    "def splitPre(pre):\n",
    "    ind = 0\n",
    "    preList = set()\n",
    "    for i in range(len(pre)):\n",
    "        if pre[i].isupper():\n",
    "            if i - ind>1:\n",
    "                word = lem(pre[ind:i].lower())\n",
    "                if word != '':\n",
    "                    preList.add(word)\n",
    "                ind = i\n",
    "    if len(preList)==0:\n",
    "        word = lem(pre.lower())\n",
    "        if word != '':\n",
    "            preList.add(word)\n",
    "    else:\n",
    "        word = lem(pre[ind:].lower())\n",
    "        if word != '':\n",
    "            preList.add(word)\n",
    "    return preList\n",
    "\n",
    "def generatePaths(path):\n",
    "    lineSplit = set()\n",
    "    for x in path:\n",
    "        x = x.translate(table)\n",
    "        if x!='' and x not in lineSplit and x not in newstopwords and x.lower() not in newstopwords:\n",
    "            word = lem(x.lower())\n",
    "            if word != '':\n",
    "                lineSplit.add(word)\n",
    "    return lineSplit\n",
    "\n",
    "\n",
    "def generateSPARQL(pline, resource):\n",
    "    qstart = 'SELECT DISTINCT '\n",
    "    qmiddle1 = 'WHERE {'\n",
    "    qmiddle2 = '} UNION {'\n",
    "    qend2 = '}. FILTER (isLiteral('\n",
    "    prevE = resource\n",
    "    query = qmiddle1\n",
    "    qstart1 = qstart\n",
    "    pathL = []\n",
    "    typeL = []\n",
    "    for pi in range(len(pline)):\n",
    "        pv = '?p' + str(pi)\n",
    "        ev = '?x' + str(pi)\n",
    "        path, answerT = pline[pi]\n",
    "        path = generatePaths(path)\n",
    "        if len(path) == 0 and len(pathL) != 0:\n",
    "            pathL.append(pathL[-1])\n",
    "        else:\n",
    "            pathL.append(path)\n",
    "        typeL.append(answerT)\n",
    "        qstart1 += pv + ' '\n",
    "        query += '{' + prevE + ' ' + pv + ' ' + ev + qmiddle2 + ev + ' ' + pv + ' ' + prevE\n",
    "        if answerT in literaltypes or answerT in timetypes:\n",
    "            query += qend2 + ev + ')).'\n",
    "        elif answerT == '':\n",
    "            query += '}.'\n",
    "        else:\n",
    "            query += '}.' + ev + ' a <http://dbpedia.org/ontology/' + answerT + '>.'\n",
    "        prevE = ev\n",
    "    query = qstart1 + query + '}'\n",
    "    print(query)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    return results, pathL, typeL\n",
    "\n",
    "def generatePreds(results):\n",
    "    predL = [[] for i in range(len(pline))]\n",
    "    predSplitL = [[] for i in range(len(pline))]\n",
    "    idL = [[] for i in range(1, len(pline))]\n",
    "    # print(len(results['results']['bindings']))\n",
    "    for t in results['results']['bindings']:\n",
    "        flag = False\n",
    "        currPreds = []\n",
    "        currPredSplits = []\n",
    "        for i in range(len(pline)):\n",
    "            currPred = t['p'+str(i)]['value']\n",
    "            if 'dbpedia.org' not in currPred or 'wikiPage' in currPred:\n",
    "                flag = True\n",
    "                break\n",
    "            currPredSplit = splitPre(currPred.split('/')[-1].split('#')[-1])\n",
    "            if len(currPredSplit) == 0:\n",
    "                flag = True\n",
    "                break\n",
    "            currPreds.append(currPred)\n",
    "            currPredSplits.append(currPredSplit)\n",
    "        if flag:\n",
    "            continue\n",
    "        prevId = -1\n",
    "        flag = False\n",
    "        for i in range(len(pline)):\n",
    "            if currPreds[i] not in predL[i]:\n",
    "                if prevId != -1:\n",
    "                    idL[i-1].append((prevId, len(predL[i])))\n",
    "                prevId = len(predL[i])\n",
    "                predL[i].append(currPreds[i])\n",
    "                predSplitL[i].append(currPredSplits[i])\n",
    "                # pvsL[i].append(splitVec(splitPre(currPreds[i].split('/')[-1].split('#')[-1])))\n",
    "            else:\n",
    "                currId = (prevId, predL[i].index(currPreds[i]))\n",
    "                if prevId != -1 and currId not in idL[i-1]:\n",
    "                    idL[i-1].append(currId)\n",
    "                prevId = currId[1]\n",
    "    return predL, predSplitL, idL\n",
    "\n",
    "def removeType(pline, h):\n",
    "    maxD = -1\n",
    "    for pi in range(len(pline)):\n",
    "        pt = pline[pi][1]\n",
    "        if pt != '':\n",
    "            depth = h.getDepth(pt)\n",
    "            if depth > maxD:\n",
    "                maxD = depth\n",
    "                maxId = pi\n",
    "    pline[maxId] = (pline[maxId][0],'')\n",
    "    return pline\n",
    "\n",
    "def computeSimVecNew(lineSplit, predSplit):\n",
    "    print(lineSplit)\n",
    "    print(predSplit)\n",
    "    lvs = np.transpose(normalize(np.stack([model[ls] for ls in lineSplit])))\n",
    "    return np.array([np.average(np.max(np.dot(normalize(np.stack([model[ps] for ps in pred])), lvs), axis=1)) for pred in predSplit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 4, 6]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2,3,6,5,4])\n",
    "b = np.array([[1,2,3],[0,4,2]])\n",
    "b[1] = a[b[1]]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper,JSON\n",
    "sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
    "sline = 'SELECT DISTINCT COUNT(?uri) WHERE { ?x <http://dbpedia.org/property/tenants> <http://dbpedia.org/resource/Toronto_Marlies> . ?x <http://dbpedia.org/property/tenants> ?uri }'\n",
    "sparql.setQuery(sline)\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "for t in results['results']['bindings']:\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
